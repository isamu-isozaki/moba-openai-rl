{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tactic_game_gym\n",
    "from stable_baselines3 import A2C, PPO\n",
    "import os\n",
    "import numpy as np\n",
    "from app.dummy_env import DummyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stable_baselines3.ppo.ppo.PPO"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "log_folder = 'test_log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('tactic_game-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_board_size = 32\n",
    "obs_shape = (obs_board_size, obs_board_size, 2*3)\n",
    "act_board_size = 2\n",
    "action_space = gym.spaces.MultiDiscrete([5 for _ in range(act_board_size*act_board_size)])\n",
    "observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=obs_shape, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DummyEnv(action_space, observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, n_steps=n_steps, tensorboard_log=log_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = 'D:/personal_projects/moba-openai-rl/ai-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step():\n",
    "    def __init__(self,file_path):\n",
    "        self.file_path = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_files = []\n",
    "for folder in os.listdir(parent_path):\n",
    "    folder_path = os.path.join(parent_path, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    for bot_folder in os.listdir(folder_path):\n",
    "        bot_folder_path = os.path.join(folder_path, bot_folder)\n",
    "        for step_file in os.listdir(bot_folder_path):\n",
    "            step_files.append(Step(os.path.join(bot_folder_path, step_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steps(n_steps):\n",
    "    np.random.shuffle(step_files)\n",
    "    return step_files[:n_steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/personal_projects/moba-openai-rl/ai-data/1\\\\0\\\\0'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_files[0].file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method OnPolicyAlgorithm.learn_from_steps of <stable_baselines3.ppo.ppo.PPO object at 0x000002EFA5C94AF0>>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn_from_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard logs saved to: test_log\\run_1\n",
      "Logging to test_log\\run_1\n",
      "-----------------------------------------\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028187027 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | -0.015      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.9        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0741     |\n",
      "|    value_loss           | 36          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03612759 |\n",
      "|    clip_fraction        | 0.435      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.43      |\n",
      "|    explained_variance   | -0.102     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.63       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.062     |\n",
      "|    value_loss           | 25.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040087085 |\n",
      "|    clip_fraction        | 0.474       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | -0.0422     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11          |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 22.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037987925 |\n",
      "|    clip_fraction        | 0.451       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | -0.0313     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 29.7        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0426725 |\n",
      "|    clip_fraction        | 0.49      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.43     |\n",
      "|    explained_variance   | -0.105    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.45      |\n",
      "|    n_updates            | 50        |\n",
      "|    policy_gradient_loss | -0.0446   |\n",
      "|    value_loss           | 28.8      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04068753 |\n",
      "|    clip_fraction        | 0.487      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.43      |\n",
      "|    explained_variance   | -0.125     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 15.6       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0529    |\n",
      "|    value_loss           | 26.6       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8800/3121421785.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_from_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ppo_mlp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\personal_projects\\moba-openai-rl\\learner-server\\stable-baselines3\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn_from_steps\u001b[1;34m(self, total_timesteps, get_steps, log_interval, callback, eval_log_path, tb_log_name)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mget_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             continue_training = self.collect_rollouts_from_files(\n\u001b[0m\u001b[0;32m    311\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                 \u001b[0msteps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\personal_projects\\moba-openai-rl\\learner-server\\stable-baselines3\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts_from_files\u001b[1;34m(self, rollout_buffer, steps)\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_obs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[1;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn_from_steps(int(1e6), get_steps, tb_log_name='ppo_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save optimized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now just save as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test_models/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load('test_models/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1e32f55e970>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseAlgorithm.predict of <stable_baselines3.a2c.a2c.A2C object at 0x000001E32F55E970>>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
